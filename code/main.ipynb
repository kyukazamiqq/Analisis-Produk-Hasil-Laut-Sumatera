{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c3391c-3aac-4b42-b286-06523d73e3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/22 19:19:13 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 172.17.0.2 instead (on interface eth0)\n",
      "25/05/22 19:19:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/22 19:19:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema DataFrame:\n",
      "root\n",
      " |-- tahun: integer (nullable = true)\n",
      " |-- kabupaten_kota: string (nullable = true)\n",
      " |-- wpp: string (nullable = true)\n",
      " |-- volume_produksi_ton: long (nullable = true)\n",
      " |-- nilai_produksi_rp_000: long (nullable = true)\n",
      " |-- provinsi: string (nullable = true)\n",
      " |-- kelompok: string (nullable = true)\n",
      " |-- jenis_ikan: string (nullable = true)\n",
      "\n",
      "\n",
      "10 Baris Pertama DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+----------+-------------------+---------------------+----------------+---------+----------+\n",
      "|tahun|     kabupaten_kota|       wpp|volume_produksi_ton|nilai_produksi_rp_000|        provinsi| kelompok|jenis_ikan|\n",
      "+-----+-------------------+----------+-------------------+---------------------+----------------+---------+----------+\n",
      "| 2023|               AGAM|WPP-RI-572|                 65|              4550000|  SUMATERA BARAT|CUMI-CUMI|CUMI-CUMI;|\n",
      "| 2023|             ASAHAN|WPP-RI-571|                872|             52328880|  SUMATERA UTARA|CUMI-CUMI|CUMI-CUMI;|\n",
      "| 2023|          BANYUASIN|WPP-RI-711|                769|             19226900|SUMATERA SELATAN|CUMI-CUMI|CUMI-CUMI;|\n",
      "| 2023|          BATU BARA|WPP-RI-571|               1400|             63005535|  SUMATERA UTARA|CUMI-CUMI|CUMI-CUMI;|\n",
      "| 2023|       DELI SERDANG|WPP-RI-571|                643|             26511140|  SUMATERA UTARA|CUMI-CUMI|CUMI-CUMI;|\n",
      "| 2023| KEPULAUAN MENTAWAI|WPP-RI-572|                 68|              2769015|  SUMATERA BARAT|CUMI-CUMI|CUMI-CUMI;|\n",
      "| 2023|KOTA BANDAR LAMPUNG|WPP-RI-572|                 59|              3849830|         LAMPUNG|CUMI-CUMI|CUMI-CUMI;|\n",
      "| 2023|  KOTA GUNUNGSITOLI|WPP-RI-572|                 34|              1386000|  SUMATERA UTARA|CUMI-CUMI|CUMI-CUMI;|\n",
      "| 2023|         KOTA MEDAN|WPP-RI-571|               2484|            135729241|  SUMATERA UTARA|CUMI-CUMI|CUMI-CUMI;|\n",
      "| 2023|        KOTA PADANG|WPP-RI-572|                478|             23440616|  SUMATERA BARAT|CUMI-CUMI|CUMI-CUMI;|\n",
      "+-----+-------------------+----------+-------------------+---------------------+----------------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Total Volume Produksi Cumi-Cumi per Provinsi:\n",
      "+----------------+------------------------+\n",
      "|        Provinsi|sum(Volume_Produksi_Ton)|\n",
      "+----------------+------------------------+\n",
      "|  SUMATERA UTARA|                   71093|\n",
      "|         LAMPUNG|                   31588|\n",
      "|  SUMATERA BARAT|                   10093|\n",
      "|SUMATERA SELATAN|                    6080|\n",
      "|            RIAU|                     739|\n",
      "|           JAMBI|                     629|\n",
      "+----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Buat SparkSession\n",
    "# SparkSession adalah entry point untuk menggunakan fungsionalitas Spark dengan DataFrame API.\n",
    "# .builder: Memulai pembangunan SparkSession.\n",
    "# .appName(\"ProduksiIkanCumiApp\"): Memberikan nama aplikasi Spark Anda.\n",
    "# .getOrCreate(): Mengembalikan SparkSession yang ada atau membuat yang baru jika belum ada.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProduksiIkanCumiApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Tentukan Path Data Parquet di HDFS\n",
    "# Ini adalah lokasi di HDFS tempat Hive menyimpan tabel Parquet Anda.\n",
    "# Anda bisa mendapatkan path ini dengan menjalankan `DESCRIBE EXTENDED data_produksi_ikan_cumi_parquet;` di Hive CLI/Beeline\n",
    "# dan mencari baris 'Location'.\n",
    "# Biasanya formatnya adalah hdfs://<namenode_host>:<port>/user/hive/warehouse/<database_name>.db/<table_name>\n",
    "# Jika Anda menggunakan kluster pseudo-distributed lokal (misalnya di Docker) dan port default HDFS,\n",
    "# ini seringkali adalah hdfs://localhost:9000 atau hanya /user/hive/warehouse/...\n",
    "# Mari kita asumsikan path yang umum:\n",
    "hdfs_path = \"hdfs://localhost:9000/user/hive/warehouse/data_produksi_ikan_cumi_parquet\"\n",
    "\n",
    "# Penting: Jika Anda tidak bisa menemukan port NameNode (biasanya 9000),\n",
    "# atau jika Anda menggunakan Docker Compose dan nama service HDFS Anda berbeda,\n",
    "# sesuaikan 'localhost:9000' dengan konfigurasi Anda.\n",
    "# Seringkali, jika HDFS dan Spark berada di lingkungan yang sama (misalnya satu Docker container besar),\n",
    "# Anda bisa cukup menggunakan path HDFS tanpa skema hdfs://<host>:<port>\n",
    "# Contoh: hdfs_path = \"/user/hive/warehouse/data_produksi_ikan_cumi_parquet\"\n",
    "# Coba yang tanpa hdfs://localhost:9000 dulu jika Anda di satu container.\n",
    "\n",
    "# 3. Baca Data Parquet dari HDFS\n",
    "# Spark akan secara otomatis mengidentifikasi format Parquet berdasarkan ekstensi file dan metadatanya.\n",
    "df_cumi = spark.read.parquet(hdfs_path)\n",
    "\n",
    "# 4. Tampilkan Schema dan Beberapa Baris Data\n",
    "# Ini akan membantu Anda memverifikasi bahwa data telah dimuat dengan benar.\n",
    "print(\"Schema DataFrame:\")\n",
    "df_cumi.printSchema()\n",
    "\n",
    "print(\"\\n10 Baris Pertama DataFrame:\")\n",
    "df_cumi.show(10)\n",
    "\n",
    "# 5. Lakukan Pengolahan Data (Contoh)\n",
    "# Anda sekarang bisa melakukan berbagai operasi Spark DataFrame.\n",
    "# Contoh: Menghitung total volume produksi cumi-cumi per provinsi\n",
    "total_volume_per_provinsi = df_cumi.groupBy(\"Provinsi\") \\\n",
    "                                   .agg({\"Volume_Produksi_Ton\": \"sum\"}) \\\n",
    "                                   .orderBy(\"sum(Volume_Produksi_Ton)\", ascending=False)\n",
    "\n",
    "print(\"\\nTotal Volume Produksi Cumi-Cumi per Provinsi:\")\n",
    "total_volume_per_provinsi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4484bf4b-955a-450f-afcd-01d95a9b6cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema DataFrame setelah VectorAssembler:\n",
      "root\n",
      " |-- tahun: integer (nullable = true)\n",
      " |-- kabupaten_kota: string (nullable = true)\n",
      " |-- wpp: string (nullable = true)\n",
      " |-- volume_produksi_ton: long (nullable = true)\n",
      " |-- nilai_produksi_rp_000: long (nullable = true)\n",
      " |-- provinsi: string (nullable = true)\n",
      " |-- kelompok: string (nullable = true)\n",
      " |-- jenis_ikan: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|features            |\n",
      "+--------------------+\n",
      "|[65.0,4550000.0]    |\n",
      "|[872.0,5.232888E7]  |\n",
      "|[769.0,1.92269E7]   |\n",
      "|[1400.0,6.3005535E7]|\n",
      "|[643.0,2.651114E7]  |\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "# Pastikan Anda sudah menjalankan kode untuk membuat SparkSession dan memuat df_cumi\n",
    "# Contoh sederhana jika Anda baru memulai atau ingin menjalankan ulang bagian ini:\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.appName(\"ProduksiIkanCumiClustering\").getOrCreate()\n",
    "# hdfs_path = \"hdfs://localhost:9000/user/hive/warehouse/data_produksi_ikan_cumi_parquet\"\n",
    "# df_cumi = spark.read.parquet(hdfs_path)\n",
    "\n",
    "# Pilih kolom-kolom numerik yang akan digunakan untuk klasterisasi\n",
    "# Kita akan gunakan Volume_Produksi_Ton dan Nilai_Produksi_Rp_000\n",
    "feature_cols = ['volume_produksi_ton', 'nilai_produksi_rp_000']\n",
    "\n",
    "# Hapus baris dengan nilai null pada kolom fitur\n",
    "df_cumi_cleaned = df_cumi.na.drop(subset=feature_cols)\n",
    "\n",
    "# Inisialisasi VectorAssembler\n",
    "# InputCols adalah kolom-kolom yang ingin digabungkan\n",
    "# OutputCol adalah nama kolom baru yang akan berisi vektor fitur\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Terapkan VectorAssembler ke DataFrame\n",
    "# Ini akan menghasilkan DataFrame baru dengan kolom 'features'\n",
    "df_features = assembler.transform(df_cumi_cleaned)\n",
    "\n",
    "print(\"Schema DataFrame setelah VectorAssembler:\")\n",
    "df_features.printSchema()\n",
    "df_features.select(\"features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5251d155-3926-4505-899c-9e30f1792282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema DataFrame setelah StandardScaler:\n",
      "root\n",
      " |-- tahun: integer (nullable = true)\n",
      " |-- kabupaten_kota: string (nullable = true)\n",
      " |-- wpp: string (nullable = true)\n",
      " |-- volume_produksi_ton: long (nullable = true)\n",
      " |-- nilai_produksi_rp_000: long (nullable = true)\n",
      " |-- provinsi: string (nullable = true)\n",
      " |-- kelompok: string (nullable = true)\n",
      " |-- jenis_ikan: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaledFeatures: vector (nullable = true)\n",
      "\n",
      "+--------------------+-----------------------------------------+\n",
      "|features            |scaledFeatures                           |\n",
      "+--------------------+-----------------------------------------+\n",
      "|[65.0,4550000.0]    |[0.043537413903869615,0.0796067957227162]|\n",
      "|[872.0,5.232888E7]  |[0.5840711526796047,0.9155460352875888]  |\n",
      "|[769.0,1.92269E7]   |[0.5150810968011652,0.33639382432551473] |\n",
      "|[1400.0,6.3005535E7]|[0.9377289148525764,1.102344781130867]   |\n",
      "|[643.0,2.651114E7]  |[0.43068549446443327,0.4638388805178748] |\n",
      "+--------------------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inisialisasi StandardScaler\n",
    "# inputCol adalah kolom fitur yang sudah digabungkan\n",
    "# outputCol adalah nama kolom baru untuk fitur yang sudah diskalakan\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False) # withMean=False untuk data sparse jika ada 0\n",
    "\n",
    "# Latih scaler pada data (menghitung rata-rata dan standar deviasi)\n",
    "scaler_model = scaler.fit(df_features)\n",
    "\n",
    "# Terapkan scaler ke DataFrame\n",
    "df_scaled = scaler_model.transform(df_features)\n",
    "\n",
    "print(\"\\nSchema DataFrame setelah StandardScaler:\")\n",
    "df_scaled.printSchema()\n",
    "df_scaled.select(\"features\", \"scaledFeatures\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e1a7f7-4dd7-4d22-96f8-5f9d698a5408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/22 19:26:36 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribusi Kluster (untuk k=3):\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0|   19|\n",
      "|         1|  138|\n",
      "|         2|    2|\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "10 Baris Pertama dengan Prediksi Kluster:\n",
      "+-----+-------------------+-------------------+---------------------+----------+\n",
      "|Tahun|     Kabupaten_Kota|Volume_Produksi_Ton|Nilai_Produksi_Rp_000|prediction|\n",
      "+-----+-------------------+-------------------+---------------------+----------+\n",
      "| 2023|               AGAM|                 65|              4550000|         1|\n",
      "| 2023|             ASAHAN|                872|             52328880|         1|\n",
      "| 2023|          BANYUASIN|                769|             19226900|         1|\n",
      "| 2023|          BATU BARA|               1400|             63005535|         1|\n",
      "| 2023|       DELI SERDANG|                643|             26511140|         1|\n",
      "| 2023| KEPULAUAN MENTAWAI|                 68|              2769015|         1|\n",
      "| 2023|KOTA BANDAR LAMPUNG|                 59|              3849830|         1|\n",
      "| 2023|  KOTA GUNUNGSITOLI|                 34|              1386000|         1|\n",
      "| 2023|         KOTA MEDAN|               2484|            135729241|         0|\n",
      "| 2023|        KOTA PADANG|                478|             23440616|         1|\n",
      "+-----+-------------------+-------------------+---------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Pusat Kluster (dalam fitur yang sudah diskalakan):\n",
      "Kluster 0: [1.79441126 1.96774239]\n",
      "Kluster 1: [0.22779726 0.24489634]\n",
      "Kluster 2: [7.49781248 6.85511509]\n"
     ]
    }
   ],
   "source": [
    "# Tentukan jumlah kluster yang diinginkan (misalnya, 3 kluster)\n",
    "k = 3\n",
    "\n",
    "# Inisialisasi model KMeans\n",
    "# featuresCol adalah kolom fitur yang akan digunakan (setelah scaling)\n",
    "# predictionCol adalah nama kolom baru yang akan berisi ID kluster hasil prediksi\n",
    "# seed untuk hasil yang reproducible\n",
    "kmeans = KMeans(featuresCol=\"scaledFeatures\", k=k, seed=1)\n",
    "\n",
    "# Latih model KMeans pada data yang sudah diskalakan\n",
    "model = kmeans.fit(df_scaled)\n",
    "\n",
    "# Dapatkan kluster hasil prediksi untuk setiap baris\n",
    "# Kolom 'prediction' akan ditambahkan ke DataFrame\n",
    "predictions = model.transform(df_scaled)\n",
    "\n",
    "print(f\"\\nDistribusi Kluster (untuk k={k}):\")\n",
    "predictions.groupBy(\"prediction\").count().orderBy(\"prediction\").show()\n",
    "\n",
    "print(\"\\n10 Baris Pertama dengan Prediksi Kluster:\")\n",
    "predictions.select(\"Tahun\", \"Kabupaten_Kota\", \"Volume_Produksi_Ton\", \"Nilai_Produksi_Rp_000\", \"prediction\").show(10)\n",
    "\n",
    "# Menampilkan pusat kluster (cluster centers)\n",
    "centers = model.clusterCenters()\n",
    "print(\"\\nPusat Kluster (dalam fitur yang sudah diskalakan):\")\n",
    "for i, center in enumerate(centers):\n",
    "    print(f\"Kluster {i}: {center}\")\n",
    "\n",
    "# Jika Anda ingin melihat pusat kluster dalam skala asli (lebih interpretif), Anda perlu \"inverse transform\"\n",
    "# Ini sedikit lebih kompleks karena StandardScaler tidak punya inverse_transform bawaan seperti scikit-learn.\n",
    "# Anda bisa secara manual mengalikan kembali dengan standar deviasi dan menambahkan mean jika withMean=True\n",
    "# Untuk tujuan ini, mari kita asumsikan interpretasi dari scaledFeatures sudah cukup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25dca6da-5461-4064-957f-323ae16fbaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Silhouette Score: 0.880911329039544\n",
      "Within Set Sum of Squared Errors (WSSSE): 45.50444148055718\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Pastikan 'predictions' DataFrame sudah ada dari langkah klasterisasi sebelumnya.\n",
    "# DataFrame ini harus memiliki kolom 'prediction' (hasil klaster) dan 'scaledFeatures' (fitur yang digunakan).\n",
    "\n",
    "# Inisialisasi evaluator untuk Silhouette Score\n",
    "evaluator = ClusteringEvaluator(\n",
    "    predictionCol=\"prediction\",  # Kolom yang berisi ID kluster hasil prediksi\n",
    "    featuresCol=\"scaledFeatures\",  # Kolom yang berisi fitur yang digunakan untuk klasterisasi\n",
    "    metricName=\"silhouette\",       # Metrik yang ingin dihitung\n",
    "    distanceMeasure=\"squaredEuclidean\" # Pengukuran jarak yang digunakan oleh K-Means\n",
    ")\n",
    "\n",
    "# Hitung Silhouette Score\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"\\nSilhouette Score: {silhouette}\")\n",
    "\n",
    "# Anda juga bisa menampilkan WSSSE (Within Set Sum of Squared Errors)\n",
    "# Ini adalah metrik yang dioptimalkan oleh K-Means, semakin kecil semakin baik.\n",
    "# Namun, tidak ada batasan bawah, jadi lebih cocok untuk membandingkan model dengan K yang berbeda.\n",
    "wssse = model.summary.trainingCost\n",
    "print(f\"Within Set Sum of Squared Errors (WSSSE): {wssse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a6bced-804b-4901-995f-43426369209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rata-rata Volume dan Nilai Produksi per Kluster:\n",
      "+----------+-----------------------+-------------------------+\n",
      "|prediction|Avg_Volume_Produksi_Ton|Avg_Nilai_Produksi_Rp_000|\n",
      "+----------+-----------------------+-------------------------+\n",
      "|0         |2679.0                 |1.1246813531578948E8     |\n",
      "|1         |340.09420289855075     |1.3997276956521738E7     |\n",
      "|2         |11194.0                |3.9181044E8              |\n",
      "+----------+-----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Pastikan SparkSession sudah dibuat dan DataFrame 'predictions' sudah tersedia\n",
    "# dari langkah klasterisasi sebelumnya. Jika tidak, Anda perlu menjalankannya lagi:\n",
    "# spark = SparkSession.builder.appName(\"ProduksiIkanCumiClusteringInterpretation\").getOrCreate()\n",
    "# hdfs_path = \"hdfs://localhost:9000/user/hive/warehouse/data_produksi_ikan_cumi_parquet\"\n",
    "# df_cumi = spark.read.parquet(hdfs_path)\n",
    "# ... (jalankan VectorAssembler, StandardScaler, dan KMeans untuk mendapatkan 'predictions') ...\n",
    "\n",
    "# Hitung rata-rata fitur asli untuk setiap kluster\n",
    "cluster_summary = predictions.groupBy(\"prediction\").agg(\n",
    "    avg(\"Volume_Produksi_Ton\").alias(\"Avg_Volume_Produksi_Ton\"),\n",
    "    avg(\"Nilai_Produksi_Rp_000\").alias(\"Avg_Nilai_Produksi_Rp_000\")\n",
    ").orderBy(\"prediction\")\n",
    "\n",
    "print(\"\\nRata-rata Volume dan Nilai Produksi per Kluster:\")\n",
    "cluster_summary.show(truncate=False)\n",
    "\n",
    "# spark.stop() # Hentikan SparkSession jika ini adalah operasi terakhir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11437cb9-ceb5-405f-bf2f-ea8d2505b056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Top 10 Kabupaten/Kota di Kluster 2 (Produksi Tinggi):\n",
      "+------------------+-----+\n",
      "|Kabupaten_Kota    |count|\n",
      "+------------------+-----+\n",
      "|KOTA TANJUNG BALAI|2    |\n",
      "+------------------+-----+\n",
      "\n",
      "---\n",
      "Top 10 Provinsi di Kluster 2 (Produksi Tinggi):\n",
      "+--------------+-----+\n",
      "|Provinsi      |count|\n",
      "+--------------+-----+\n",
      "|SUMATERA UTARA|2    |\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Pastikan SparkSession sudah dibuat dan DataFrame 'predictions' sudah tersedia\n",
    "# Jika Anda baru memulai atau ingin menjalankan ulang bagian ini, Anda perlu menjalankan\n",
    "# langkah-langkah sebelumnya (load data, feature engineering, scaling, clustering)\n",
    "# untuk mendapatkan DataFrame 'predictions'.\n",
    "\n",
    "# Filter DataFrame hanya untuk Kluster 2 (Produksi Tinggi)\n",
    "df_cluster2 = predictions.filter(col(\"prediction\") == 2)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Top 10 Kabupaten/Kota di Kluster 2 (Produksi Tinggi):\")\n",
    "# Hitung jumlah entri per Kabupaten/Kota dalam Kluster 2\n",
    "kab_kota_cluster2 = df_cluster2.groupBy(\"Kabupaten_Kota\").count().orderBy(col(\"count\").desc())\n",
    "kab_kota_cluster2.show(10, truncate=False)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Top 10 Provinsi di Kluster 2 (Produksi Tinggi):\")\n",
    "# Hitung jumlah entri per Provinsi dalam Kluster 2\n",
    "provinsi_cluster2 = df_cluster2.groupBy(\"Provinsi\").count().orderBy(col(\"count\").desc())\n",
    "provinsi_cluster2.show(10, truncate=False)\n",
    "\n",
    "# Jangan lupa menghentikan SparkSession jika ini adalah operasi terakhir Anda\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "330c5a0d-8dc7-4819-89c8-f597ec17c828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menyimpan seluruh DataFrame 'predictions' ke /home/jovyan/work/output_cumi_parquet_all_clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selesai.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Pastikan SparkSession sudah dibuat dan DataFrame 'predictions' tersedia.\n",
    "# Contoh:\n",
    "# spark = SparkSession.builder.appName(\"ExportLocalParquet\").getOrCreate()\n",
    "# asumsi 'predictions' adalah DataFrame yang berisi data cumi-cumi Anda dengan kolom 'prediction'\n",
    "\n",
    "# Tentukan jalur output di filesystem lokal container JupyterLab Anda.\n",
    "# Misalnya, di dalam folder /home/jovyan/work/output_parquet\n",
    "# Atau, jika Anda mau, bisa langsung di root folder kerja Anda: \"./output_cumi_parquet\"\n",
    "local_output_path = \"/home/jovyan/work/output_cumi_parquet\"\n",
    "\n",
    "# --- OPSI 1: Menyimpan seluruh hasil klasterisasi ke Parquet lokal ---\n",
    "print(f\"Menyimpan seluruh DataFrame 'predictions' ke {local_output_path}_all_clusters...\")\n",
    "predictions.write.mode(\"overwrite\").parquet(f\"{local_output_path}_all_clusters\")\n",
    "print(\"Selesai.\")\n",
    "# Jangan lupa menghentikan SparkSession jika sudah selesai dengan semua operasi.\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68d7b250-517d-42fd-bb68-50a65e832337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direktori model '/home/jovyan/work/kmeans_cumi_model' belum ada. Akan dibuat.\n",
      "Model K-Means berhasil disimpan di lokal: /home/jovyan/work/kmeans_cumi_model\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeansModel # Ini penting untuk memuat model nanti\n",
    "import os\n",
    "\n",
    "# Pastikan objek 'model' (model KMeans yang sudah dilatih) tersedia di sesi Jupyter Anda.\n",
    "# Jika Anda menjalankan cell ini terpisah, Anda perlu memastikan 'model' sudah didefinisikan\n",
    "# dari langkah klasterisasi sebelumnya.\n",
    "\n",
    "# Tentukan jalur di filesystem lokal container JupyterLab Anda untuk menyimpan model\n",
    "# Misalnya, di dalam folder /home/jovyan/work/models\n",
    "local_model_path = \"/home/jovyan/work/kmeans_cumi_model\"\n",
    "\n",
    "# --- Opsional: Hapus folder model lama jika sudah ada ---\n",
    "# Ini penting karena Spark tidak akan menimpa folder secara default\n",
    "# dan akan memberikan error jika folder sudah ada.\n",
    "# os.path.exists adalah fungsi Python standar untuk memeriksa keberadaan file/folder.\n",
    "if os.path.exists(local_model_path):\n",
    "    print(f\"Direktori model '{local_model_path}' sudah ada. Menghapus...\")\n",
    "    # Menggunakan shutil.rmtree untuk menghapus folder secara rekursif\n",
    "    # Pastikan Anda mengimpor 'shutil' jika ingin menggunakan ini\n",
    "    import shutil\n",
    "    shutil.rmtree(local_model_path)\n",
    "    print(\"Direktori model lama berhasil dihapus.\")\n",
    "else:\n",
    "    print(f\"Direktori model '{local_model_path}' belum ada. Akan dibuat.\")\n",
    "\n",
    "# --- Simpan Model K-Means ke Filesystem Lokal ---\n",
    "model.save(local_model_path)\n",
    "print(f\"Model K-Means berhasil disimpan di lokal: {local_model_path}\")\n",
    "\n",
    "# Jangan lupa menghentikan SparkSession jika sudah selesai dengan semua operasi.\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a74501bc-f3ab-43e5-b82c-c3d73e287404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/hadoop-3.4.1/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/apache-tez-0.10.4-bin/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "put: `/home/jovyan/work/output_cumi_parquet': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Ganti 'localhost:9000' dengan host dan port NameNode HDFS Anda yang sebenarnya.\n",
    "# Jika Anda menggunakan Docker Compose, kemungkinan ini adalah nama service HDFS (misal: namenode:9000).\n",
    "HDFS_NAMENODE_URI=\"hdfs://localhost:9000\"\n",
    "\n",
    "# Jalur lokal di dalam container JupyterLab\n",
    "LOCAL_SOURCE_PATH=\"/home/jovyan/work/output_cumi_parquet\"\n",
    "\n",
    "# Jalur tujuan di HDFS\n",
    "HDFS_DEST_PATH=\"/output\" # Nama folder tujuan di HDFS\n",
    "\n",
    "# Perintah untuk mengunggah folder\n",
    "!hdfs dfs -put -f \"$LOCAL_SOURCE_PATH\" \"$HDFS_DEST_PATH\"\n",
    "# -f : Opsional, untuk menimpa file/folder tujuan jika sudah ada. Hati-hati menggunakannya!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54c4ae-69c4-4766-ad68-11eda37a9ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
